
---
date: 2024-08-29T14:00:00
title: Highlights for the week ending 2024-08-29
changes_categories:
  - Highlights
owner:
  - https://github.com/orgs/giantswarm/teams/sig-product
---

## General

<!-- This where BREAKING CHANGES ARE HIGHLIGHTED -->

## Apps

- [logging-operator](https://github.com/giantswarm/logging-operator) 
  - [0.8.0](https://github.com/giantswarm/logging-
operator/compare/v0.7.3...v0.8.0) 
      * Add helm chart templating test in ci pipeline.
      * Add tests with ats in ci pipeline.
- [prometheus-rules](https://github.com/giantswarm/prometheus-rules) 
  - [4.12.0](https://github.com/giantswarm/prometheus-
rules/compare/v4.11.0...v4.12.0) 
      * Add CAPI cluster namespace to recording rule `aggregation:giantswarm:cluster_info` for use by [`resource-police`](https://github.com/giantswarm/resource-police/) to find out to whom each test cluster belongs
  - [4.11.0](https://github.com/giantswarm/prometheus-
rules/compare/v4.10.0...v4.11.0) 
      * Assign alerts on core components directly to turtles.
      * Ignore new `watchdog` collector of node-exporter since our clusters will not have data for these devices and therefore the `node_scrape_collector_success` metric would be 0 


- [karpenter-app](https://github.com/giantswarm/karpenter-app) 
  - [0.13.0](https://github.com/giantswarm/karpenter-
app/compare/v0.12.1...v0.13.0) 
      * Detect and use latest `flowcontrol` API version.
      * Update PolicyExceptions to v2 and fallback to v2beta1.
- [cluster-vsphere](https://github.com/giantswarm/cluster-vsphere) 
  - [0.61.0](https://github.com/giantswarm/cluster-
vsphere/compare/v0.60.1...v0.61.0) 
> [!WARNING] This release adds all default apps to cluster-vsphere, so
> default-apps-vsphere App is not used anymore. Changes in cluster-vsphere are
> breaking and cluster upgrade requires manual steps where default-apps-
> vsphere App is removed before upgrading cluster-vsphere. See details below.
      * Render capi-node-labeler App CR from cluster chart.
      * Render cert-exporter App CR from cluster chart and add vSphere-specific cert-exporter config.
      * Render cert-manager App CR from cluster chart and add vSphere-specific cert-manager config.
      * Render chart-operator-extensions App CR from cluster chart.
      * Render cilium HelmRelease CR from cluster chart and add vSphere-specific cilium config.
      * Render cilium-servicemonitors App CR from cluster chart.
      * Render coredns HelmRelease CR from cluster chart.
      * Render etc-kubernetes-resources-count-exporter App CR from cluster chart.
      * Render k8s-dns-node-cache App CR from cluster chart.
      * Render metrics-server App CR from cluster chart.
      * Render net-exporter App CR from cluster chart.
      * Render network-policies HelmRelease CR from cluster chart and add vSphere-specific network-policies config.
      * Render node-exporter App CR from cluster chart and add vSphere-specific node-exporter config.
      * Render observability-bundle App CR from cluster chart.
      * Render observability-policies App CR from cluster chart.
      * Render security-bundle App CR from cluster chart.
      * Render teleport-kube-agent App CR from cluster chart.
      * Render vertical-pod-autoscaler App CR from cluster chart.
      * Render vertical-pod-autoscaler-crd HelmRelease CR from cluster chart.
      * Render HelmRepository CRs from cluster chart.
      * Add missing Helm value .Values.global.controlPlane.apiServerPort.
      * Add Makefile `template` target that renders manifests with CI values from the chart.
      * Add Makefile `generate` target that normalizes and validates schema, generates docs and Helm values, and updates Helm dependencies.
      * Remove cilium HelmRelease.
      * Remove coredns HelmRelease.
      * Remove network-policies HelmRelease.
      * Remove HelmRepository CRs.
### ⚠️ Workload cluster upgrade with manual steps
The steps to upgrade a workload cluster, with unifying cluster-vsphere and
default-apps-vsphere, are the following: \- Upgrade default-apps-vsphere App
to the v0.16.0 release. \- Update default-apps-vsphere Helm value
`.Values.deleteOptions.moveAppsHelmOwnershipToClusterVSphere` to `true`. \-
All App CRs, except observability-bundle and security-bundle, will get `app-
operator.giantswarm.io/paused: true` annotation, so wait few minutes for Helm
post-upgrade hook to apply the change to all required App CRs. \- Delete
default-apps-vsphere CR. \- ⚠️ In case you are removing default-apps-vsphere
App CR from your gitops repo which is using Flux, and depending on how Flux is
configured, default-apps-vsphere App CR may or may not get deleted from the
management cluster. In case Flux does not delete default-apps-vsphere App CR
from the management cluster, make sure to delete it manually. \- App CRs (on
the MC) for all default apps will get deleted. Wait few minutes for this to
happen. \- Chart CRs on the workload cluster will remain untouched, so all
apps will continue running. \- Upgrade cluster-vsphere App CR to the v0.61.0
release. \- cluster-vsphere will deploy all default apps, so wait a few
minutes for all Apps to be successfully deployed. \- Chart resources on the
workload cluster will get updated, as newly deployed App resources will take
over the reconciliation of the existing Chart resources.
We're almost there, with just one more issue to fix manually.
VPA CRD used to installed as an App resource from default-apps-vsphere, and
now it's being installed as a HelmRelease from cluster-vsphere. Now, as a
consequence of the above upgrade, we have the following situation: \- default-
apps-vsphere App has been deleted, but the vertical-pod-autoscaler-crd Chart
CRs remained in the workload cluster. \- cluster-vsphere has been upgraded, so
now it also installs vertical-pod-autoscaler-crd HelmRelease. \- outcome: we
now have vertical-pod-autoscaler-crd HelmRelease in the MC and vertical-pod-
autoscaler-crd Chart CR in the WC.
Now we will remove the leftover vertical-pod-autoscaler-crd Chart CR in a safe
way:
  1. Pause vertical-pod-autoscaler-crd Chart CR.
Add annotation `chart-operator.giantswarm.io/paused: "true"` to vertical-pod-
autoscaler-crd Chart CR in the workload cluster:
`sh kubectl annotate -n giantswarm chart vertical-pod-autoscaler-crd chart-
operator.giantswarm.io/paused="true" --overwrite`
  1. Delete vertical-pod-autoscaler-crd Chart CR in the workload cluster.
`shell kubectl delete -n giantswarm chart vertical-pod-autoscaler-crd`
The command line will probably hang, as the chart-operator finalizer has is
not getting removed (vertical-pod-autoscaler-crd Chart CR has been paused).
Proceed to the next step to remove the finalizer and unblock the deletion.
  1. Remove finalizers from the vertical-pod-autoscaler-crd Chart CR
Open another terminal window and run the following command to remove the
vertical-pod-autoscaler-crd Chart CR finalizers:
`shell kubectl patch chart vertical-pod-autoscaler-crd -n giantswarm
--type=json -p='[{"op": "remove", "path": "/metadata/finalizers"}]'`
This will unblock the deletion and vertical-pod-autoscaler-crd will get
removed, **without actually deleting VPA CustomResourceDefinition**.
From now on, VPA CustomResourceDefinition will be maintained by the vertical-
pod-autoscaler HelmRelease on the management cluster.
  - [0.60.1](https://github.com/giantswarm/cluster-
vsphere/compare/v0.60.0...v0.60.1) 
      * Rename caFile to caPem in values schema. 


- [default-apps-vsphere](https://github.com/giantswarm/default-apps-vsphere) 
  - [0.16.0](https://github.com/giantswarm/default-apps-
vsphere/compare/v0.15.0...v0.16.0) 
> [!WARNING] This release includes changes that enable the unification of
> cluster-vsphere and default-apps-vsphere. The unification of cluster-vsphere
> and default-apps-vsphere does not happen automatically, it must be enabled
> explicitly and even then it requires manual steps. See details below.
      * Add `observability-policies` app.
      * New Helm value `.Values.deleteOptions.moveAppsHelmOwnershipToClusterVSphere`, which, when enabled, will pause all apps in the default-apps-vsphere, and it will enable the below hooks. The apps are paused in order to prevent the deletion of Chart resources in the WC.
      * Helm hook to remove app-operator finalizer from App CRs, so that App CRs are deleted from the MC, while Chart CRs stay on the WC.
      * Helm hook to propagate pause annotation to all bundled apps.
      * Update `net-exporter` to v1.21.0.
      * Update `k8s-dns-node-cache-app` to v2.8.1.
      * Update `teleport-kube-agent-app` to v0.9.1.
      * Update `cert-manager-app` to v3.8.0.
      * Update `cert-exporter` to v2.9.1.
      * Update `teleport-kube-agent-app` to v0.9.2.
      * Update `observability-bundle` to v1.5.1.
      * Update `security-bundle` to v1.8.0.
      * Update `vertical-pod-autoscaler-app` to v5.2.4.
### ⚠️ Workload cluster upgrade with manual steps
The steps to upgrade a workload cluster, with unifying cluster-vsphere and
default-apps-vsphere, are the following: \- Upgrade default-apps-vsphere App
to the latest release that includes this change. \- Update default-apps-
vsphere Helm value
`.Values.deleteOptions.moveAppsHelmOwnershipToClusterVSphere` to `true`. \-
All App CRs, except observability-bundle and security-bundle, will get `app-
operator.giantswarm.io/paused: true` annotation, so wait few minutes for Helm
post-upgrade hook to apply the change to all required App CRs. \- Delete
default-apps-vsphere CR. \- ⚠️ In case you are removing default-apps-vsphere
App CR from your gitops repo which is using Flux, and depending on how Flux is
configured, default-apps-vsphere App CR may or may not get deleted from the
management cluster. In case Flux does not delete default-apps-vsphere App CR
from the management cluster, make sure to delete it manually. \- App CRs (on
the MC) for all default apps will get deleted. Wait few minutes for this to
happen. \- Chart CRs on the workload cluster will remain untouched, so all
apps will continue running. \- Upgrade cluster-vsphere App CR to the latest
(TBA release which includes [these
changes](https://github.com/giantswarm/cluster-vsphere/pull/262)). \- cluster-
vsphere will deploy all default apps, so wait a few minutes for all Apps to be
successfully deployed. \- Chart resources on the workload cluster will get
updated, as newly deployed App resources will take over the reconciliation of
the existing Chart resources.
We're almost there, with just one more issue to fix manually.
VPA CRD used to installed as an App resource from default-apps-vsphere, and
now it's being installed as a HelmRelease from cluster-vsphere. Now, as a
consequence of the above upgrade, we have the following situation: \- default-
apps-vsphere App has been deleted, but the vertical-pod-autoscaler-crd Chart
CRs remained in the workload cluster. \- cluster-vsphere has been upgraded, so
now it also installs vertical-pod-autoscaler-crd HelmRelease. \- outcome: we
now have vertical-pod-autoscaler-crd HelmRelease in the MC and vertical-pod-
autoscaler-crd Chart CR in the WC.
Now we will remove the leftover vertical-pod-autoscaler-crd Chart CR in a safe
way:
  1. Pause vertical-pod-autoscaler-crd Chart CR.
Add annotation `chart-operator.giantswarm.io/paused: "true"` to vertical-pod-
autoscaler-crd Chart CR in the workload cluster:
`sh kubectl annotate -n giantswarm chart vertical-pod-autoscaler-crd chart-
operator.giantswarm.io/paused="true" --overwrite`
  1. Delete vertical-pod-autoscaler-crd Chart CR in the workload cluster.
`shell kubectl delete -n giantswarm chart vertical-pod-autoscaler-crd`
The command line will probably hang, as the chart-operator finalizer has is
not getting removed (vertical-pod-autoscaler-crd Chart CR has been paused).
Proceed to the next step to remove the finalizer and unblock the deletion.
  1. Remove finalizers from the vertical-pod-autoscaler-crd Chart CR
Open another terminal window and run the following command to remove the
vertical-pod-autoscaler-crd Chart CR finalizers:
`shell kubectl patch chart vertical-pod-autoscaler-crd -n giantswarm
--type=json -p='[{"op": "remove", "path": "/metadata/finalizers"}]'`
This will unblock the deletion and vertical-pod-autoscaler-crd will get
removed, **without actually deleting VPA CustomResourceDefinition**.
From now on, VPA CustomResourceDefinition will be maintained by the vertical-
pod-autoscaler HelmRelease on the management cluster.
- [dns-operator-route53](https://github.com/giantswarm/dns-operator-route53) 
  - [0.9.2](https://github.com/giantswarm/dns-operator-
route53/compare/v0.9.1...v0.9.2) 
      * Add rbac rule for operator to access infraClusters on CAPA.

## Docs

<!-- FER is filling this one -->
